# Physical Law

> How Far is Video Generation from World Model: A Physical Law Perspective
> https://arxiv.org/abs/2411.02385<br>

## Abstract

1.  研究背景与目标
   - OpenAI的Sora展示了视频生成在开发符合基本物理定律的世界模型(world model)方面的潜力.
   - 然而, 视频生成模型(video generation models)能否在没有人为先验(human priors)的情况下, 仅从视觉数据(visual data)中发现这些物理定律值得质疑.
   - 一个学习了真实物理定律的世界模型应能对细微差异具有鲁棒性(robustness), 并且能够在未见过的场景(unseen scenarios)中做出正确的推断.


2.  研究问题
   - 在本工作中, 我们在三个关键场景下评估了视频生成模型的表现: 
     1.  分布内场景(in-distribution): 模型在其训练数据分布内的表现.
     2.  分布外场景(out-of-distribution): 模型在训练数据分布之外的场景中的表现.
     3.  组合泛化(combinatorial generalization): 模型是否能够组合已有的知识来泛化到新的组合场景.

3.  方法与实验平台
   - 我们开发了一个二维(2D)模拟测试平台, 用于模拟物体运动和碰撞, 以生成完全由一个或多个经典力学定律(classical mechanics laws)确定的视频.
   - 该平台可以生成无限的数据, 为大规模实验提供支持, 并能够对生成的视频是否遵循物理定律进行定量评估(quantitative evaluation).
   - 我们训练了基于扩散模型(diffusion-based)的视频生成模型, 根据初始帧(initial frames)来预测物体的运动.

4.  实验结果
   - 扩展实验(scaling experiments)的结果表明: 
     1.  分布内泛化(in-distribution generalization): 模型在分布内表现出完美的泛化能力.
     2.  组合泛化(combinatorial generalization): 在组合泛化方面, 模型表现出可测量的扩展行为(scaling behavior).
     3.  分布外泛化失败(out-of-distribution failure): 模型在分布外场景中表现较差, 无法成功泛化.

5.  进一步实验与关键见解
   - 我们进行了进一步的实验, 并得到了关于模型泛化机制的两个关键见解: 
     1.  基于案例的泛化行为(case-based generalization behavior): 模型未能抽象出普遍的物理规则, 而是表现出“基于案例”的泛化行为, 即倾向于模仿最接近的训练样本(closest training example).
     2.  优先级顺序: 在泛化到新的场景时, 模型在参考训练数据时优先考虑不同的因素, 优先级为: 颜色(color) > 尺寸(size) > 速度(velocity) > 形状(shape).

6.  结论
   - 我们的研究表明, 尽管在Sora的成功中扩展模型规模起到了重要作用, 但仅依靠规模扩展不足以使视频生成模型发现基本物理定律.


## 1 INTRODUCTION

1.  基础模型的兴起
   - 基础模型(Foundation models)(Bommasani et al. , 2021)通过将模型和数据扩展到前所未有的规模, 展现了惊人的能力(Brown, 2020; Kaplan et al. , 2020).
   - 例如, OpenAI的Sora(Brooks et al. , 2024)不仅能生成高保真、超现实的视频, 还引发了对世界模型(world models)研究的新热潮(Yang et al. , 2023).

2.  视频生成与物理世界模拟
   - Sora报告(Brooks et al. , 2024)中指出: “扩展视频生成模型是构建物理世界通用模拟器(general-purpose simulators)的有前景的路径. ”
   - 世界模拟器(World simulators)因其生成逼真数据和提供精确模拟的能力, 正在机器人学(robotics)(Yang et al. , 2023)和自动驾驶(autonomous driving)(Hu et al. , 2023)领域引起广泛关注.
   - 这些模型需要理解基本的物理定律, 以产生超越训练语料(training corpus)范围的数据, 并保证精确的模拟. 然而, 视频生成是否可以仅通过观察视频来发现这些规则(正如Sora所做的)仍然是一个未解的问题.
   - 我们的目标是进行系统研究, 以理解扩展模型在物理定律发现中的关键作用和局限性.

3.  模型泛化与物理法则学习的挑战
   - 要确定视频生成模型是否真正学习了一种物理法则, 而不仅仅是记住了数据, 这具有很大挑战性.
   - 由于模型的内部知识是不可访问的, 我们只能通过考察模型在未见过场景(unseen scenarios)中的预测——即它的泛化能力(generalization ability)——来推断模型的理解程度.
   - 我们在本文中提出了一种分类方法(如图1所示), 用于基于训练数据和测试数据之间的关系进行全面评估.
     - 分布内泛化(In-distribution, ID generalization): 假设训练数据和测试数据是独立同分布(i. i. d. ).
     - 分布外泛化(Out-of-distribution, OOD generalization): 指的是模型在来自不同于训练数据分布的测试数据上的表现, 尤其是在隐含参数(latent parameters)超出训练时的范围的情况下.
       - >(Latent Parameters)是指在数据中存在但通常不可直接观测或测量的变量, 它们描述了数据的内在结构或特征.
         > - 在许多模型中, 数据的可见特征可能由一些隐藏的、不可见的因素所驱动. 隐含参数正是这些隐藏的因素.
         > - 隐含参数通常是原始数据的低维度表示, 它们用于捕获数据的关键特征和关系. 例如, 在深度学习的自编码器(Autoencoder)中, 隐含参数表示数据通过编码器后的压缩表示, 保留了描述数据本质的重要信息.
     - 人类水平的物理推理可以轻松地进行分布外泛化(OOD extrapolation), 并预测物理过程, 而无需遇到完全相同的场景.
     - 另外, 我们还考察了一种特殊的分布外能力, 称为组合泛化(combinatorial generalization), 它评估模型是否可以以新颖的方式结合两个不同的概念, 这一特性通常被认为是基础模型在迈向人工通用智能(AGI)过程中不可或缺的(Du & Kaelbling, 2024).
     - > 分布外泛化关注的是数据分布的变化, 而组合泛化关注的是已知概念的新组合. <br>
       > - 分布外泛化: 如果一个模型在看到训练数据中存在的某个小球在左边滚动, 那么测试它的分布外泛化能力可能是考察它在看到小球从右边滚动时的表现. 这里的挑战在于小球的运动方向不同于训练数据, 但小球的其他特征保持不变. <br>
       > - 组合泛化: 如果模型见过红色的小球和蓝色的方块, 组合泛化会评估模型是否可以理解蓝色的小球或红色的方块. 这要求模型理解“颜色”和“形状”是可以独立组合的, 而不仅仅是记住原有的固定组合.

<figure style="display: block; text-align: center;">   <img src="MLLM/2024_12/images/2024-11-26-19-40-35.png" at="name" style="display: block; margin: auto; width: 100%; height: auto;"></figure>

1.  复杂对象与运动的挑战
   - 实际视频通常包含复杂的、非刚性(non-rigid)对象和运动, 这对定量评估(quantitative evaluation)甚至人工验证(human validation)来说都是重大挑战.
   - 这类视频中的丰富纹理和外观可能成为混杂因素(confounding factors), 使模型分散注意力, 难以专注于物理规律本身.
   - 为了减轻这些问题的影响, 我们特别聚焦于经典力学(classical mechanics), 并开发了2D模拟器, 其中对象由简单的几何形状表示. 每个视频展示这些二维对象的运动或碰撞, 其行为完全由一个或两个基本物理定律决定, 给定初始帧.
   - 该模拟器使我们能够生成大规模数据集, 以支持视频生成模型的scaling.
   - 此外, 我们开发了一种工具, 用于从像素中推断生成视频中每个对象的内部状态(internal states)(例如位置和大小), 从而为物理定律发现建立定量评估指标.

2.  扩展实验与结果
   - 我们首先研究了扩展视频生成模型如何影响分布内(ID)和分布外(OOD)的泛化能力.
   - 我们选择了三个基本的物理定律进行模拟: 小球的匀速直线运动(uniform linear motion)、两个小球的完全弹性碰撞(perfectly elastic collision)、以及小球的抛物线运动(parabolic motion).
   - 我们将数据集从3万(30K)例扩展到300万(3 million)例, 同时将视频扩散模型的参数量从2200万(22M)增加到3. 1亿(310M).
   - 结果显示, 模型在所有任务中均实现了接近完美的分布内泛化(ID generalization). 然而, 分布外泛化(OOD generalization)的错误并未随着数据量和模型规模的增加而得到改善, 暴露出扩展视频生成模型在处理分布外数据方面的局限性.
   - 对于组合泛化(combinatorial generalization), 我们设计了一个环境, 涉及多个对象的自由落体和碰撞, 以研究它们的相互作用. 每次从八个对象中选取四个来创建一个视频, 共有70种组合(C4_8)可能性. 我们使用其中的60种进行训练, 10种用于测试.
   - 我们通过改变训练数据量(从60万到600万)来训练模型. 我们对生成的测试样本进行手动评估, 如果视频在物理上看起来不合理, 则将其标记为“异常”(abnormal).
   - 实验结果表明, 扩展数据量显著降低了异常案例的百分比, 从67%降至10%. 这表明扩展对于改善组合泛化至关重要.

3.  泛化机制的深入分析
   - 我们的实证分析揭示了视频生成模型泛化机制的两个有趣属性: 
     1.  基于案例的泛化偏差: 
        - 这些模型容易受到来自训练集的“欺骗性”("deceptive")示例的影响, 在某些情况下表现出基于案例(case-based)的泛化行为.
        - 这种现象也出现在大型语言模型(large language models)(Hu et al. , 2024)中, 描述了模型在解决新任务时倾向于参考相似训练案例的倾向.
        - 例如, 考虑一个训练有高速度小球匀速直线运动数据的视频模型. 如果通过水平翻转(horizontal flipping)视频进行数据增强, 从而引入反向运动, 那么该模型可能会生成一个低速小球在初始帧后反向运动的场景, 即便这种行为在物理上并不正确.
        - > 所以说, 泛化程度不够的时候, 数据增强可能会引入违背物理现实的情况
     2.  数据属性的竞争优先级: 
        - 我们还探讨了不同数据属性在泛化过程中如何竞争. 例如, 如果用于匀速运动训练的数据由红色小球和蓝色方块组成, 那么模型可能会在条件帧后将一个红色方块立即变成小球.
        - 这种行为表明, 模型优先考虑颜色(color)而非形状(shape). 我们的成对分析(pairwise analysis)揭示了以下优先级层次: 颜色 > 尺寸(size) > 速度(velocity) > 形状.
        - 这一排序可能解释了为什么当前的视频生成模型通常在保持对象一致性方面存在困难.

## 2 DISCOVERING PHYSICS LAWS WITH VIDEO GENERATION

### 2. 1 PROBLEM DEFINITION

1.  研究目标与框架
   - 在本节中, 我们旨在建立一个框架, 并定义在视频生成背景下物理定律发现的概念.
   - 在经典物理学中, 物理定律通过数学方程表达, 这些方程能够从初始条件预测未来状态和动态.
   - 在基于视频的观察领域, 每一帧代表时间中的一个时刻, 而物理定律的预测对应于在给定过去状态的条件下生成未来的帧.

2.  物理过程与潜变量定义
   - 考虑一个涉及若干潜在变量(latent variables)的物理过程, 这些潜变量表示为 \( z =(z_1, z_2, \dots, z_k) \in Z \subseteq \mathbb{R}^k \), 每个变量代表某个物理参数, 如速度(velocity)或位置(position).
   - 根据经典力学, 这些潜变量将通过微分方程 \( \dot{z} = F(z) \) 进行演化.
   - 在离散形式中, 如果两个连续帧之间的时间间隔为 \( \delta \), 则有 \( z_{t+1} \approx z_t + \delta F(z_t) \).
   - 我们将渲染函数(rendering function)记作 \( R(\cdot) : Z \to \mathbb{R}^{3 \times H \times W} \), 用于将世界状态渲染为形状为 \( H \times W \) 的 RGB 图像.
   - 考虑一个由 \( L \) 帧组成的视频 \( V = \{I_1, I_2, \dots, I_L\} \), 其遵循经典力学动态.
   - 物理一致性(physical coherence)要求存在一系列满足以下条件的潜变量: 
     1.  \( z_{t+1} = z_t + \delta F(z_t), \quad t = 1, \dots, L - 1 \)
     2.  \( I_t = R(z_t), \quad t = 1, \dots, L \)

3.  视频生成模型的训练与物理一致性
   - 我们训练一个由参数 \( \theta \) 表示的视频生成模型 \( p \), 其中 \( p_\theta(I_1, I_2, \dots, I_L) \) 描述了模型对视频帧的理解.
   - 我们可以通过从条件概率分布 \( p_\theta(I'_{c+1}, \dots, I'_L \mid I_1, \dots, I_c) \) 采样, 基于初始帧的条件预测后续帧.
   - 变量 \( c \) 通常取值为 1 或 3, 具体取决于任务的要求.
   - 因此, 物理一致性损失(physical-coherence loss)可以简单地定义为 \( -\log p_\theta(I_{c+1}, \dots, I_L \mid I_1, \dots, I_c) \).
     - 它衡量了预测值符合真实世界发展的可能性.
     - 要准确预测后续帧, 模型必须理解潜在的物理过程. 我们可以通过定量评估来判断视频生成模型是否正确发现并模拟了物理定律.

### 2.2 VIDEO GENERATION MODEL

1. 视频生成模型架构
   - 根据Sora(Brooks et al., 2024), 我们采用了变分自编码器(Variational Auto-Encoder, [VAE](/MLLM/2024_12/VAE.md))和[DiT](/MLLM/2024_12/DiT.md)架构来进行视频生成.
   - VAE在空间和时间上将视频压缩为潜在表示, 而DiT则对去噪过程进行建模.
   - 这种方法展示了强大的扩展性, 并在生成高质量视频方面取得了有希望的结果.

2. VAE模型
   - 我们使用了一个(2+1)D-VAE将视频投影到一个潜在空间中.
   - 从SD1.5-VAE结构开始, 我们使用3D块将其扩展为一个时空自编码器(spatiotemporal autoencoder)(Yu et al., 2023b).
   - (2+1)D-VAE的所有参数均在高质量的图像和视频数据上进行了预训练, 以保持强大的外观建模能力, 同时实现运动建模.
   - 更多详细信息请参考附录A.3.1.
   - 在本文中, 我们固定预训练的VAE编码器, 将其用作视频压缩器.
   - 附录A.3.2中的结果确认了VAE能够准确编码和解码物理事件视频, 这使得我们可以专注于训练扩散模型以学习物理定律.

3. 扩散模型(Diffusion model)
   - 给定由VAE模型压缩得到的潜在表示, 我们将其展平为一个时空块序列, 作为Transformer的token.
   - 值得注意的是, 自注意力机制(self-attention)被应用于整个视频token的时空序列, 不区分空间和时间维度.
   - 对于位置嵌入(positional embedding), 我们采用了RoPE的3D变体(Su et al., 2024).
   - 如第2.1节所述, 我们的视频模型以前c帧为条件(condition). c帧的视频通过零填充(zero-padded)至与完整物理视频相同的长度.
   - 我们还引入了一个二进制掩码“video”, 将前c帧的值设置为1, 表示这些帧是条件输入.
   - 噪声、条件和掩码视频沿通道维度连接, 形成模型的最终输入.

### 2.3 ON THE VERIFICATION OF LEARNED LAWS

1. 验证学习到的物理定律
   - 假设我们已经基于上述公式学习了一个视频生成模型, 如何确定其是否发现了潜在的物理定律呢？
   - 一个成熟的物理定律描述了自然界的行为, 例如物体如何运动和相互作用. 因此, 包含真实物理定律的视频模型应能经受实验验证, 在任何情况下都能生成合理的预测, 这展示了模型的泛化能力(generalization ability).
   - 为了全面评估这一点, 我们在本文的范围内考虑了以下泛化分类(见图1): 
     1. 分布内泛化(In-distribution, ID generalization): 描述了训练数据和测试数据来自相同分布的情形. 在我们的案例中, 训练数据和测试数据遵循相同的物理定律, 且位于相同域中.
     2. 分布外泛化(Out-of-distribution, OOD generalization): 
        - 人类一旦学会了一个物理定律, 就可以轻松推测出以前从未观察过的场景. 这种能力称为分布外泛化(OOD generalization).
        - 尽管听起来具有挑战性, 但这种评估是必要的, 因为它表明模型是否可以从数据中学习到原则性规则.
     3. 组合泛化(Combinatorial generalization): 
        - 还有一种介于ID和OOD之间的情况, 更具实际价值.
        - 我们称之为组合泛化, 代表了一种场景, 其中每个“概念”或对象在训练期间都已观察过, 但并不是它们的每一种组合都被观察过.
        - 它检验模型是否能够以新的方式有效地组合来自过去经验的相关信息.
        - 类似的概念已在大型语言模型(LLMs)中被探讨(Riveland & Pouget, 2024), 研究表明, 模型可以通过重新组合先前学习到的组件, 在没有任务特定经验的情况下, 在语言指令任务上表现出色.

## 3 IN-DISTRIBUTION AND OUT-OF-DISTRIBUTION GENERALIZATION

在本节中, 我们研究了分布内(in-distribution, ID)和分布外(out-of-distribution, OOD)泛化与模型或数据扩展的相关性. 我们专注于由基本运动学方程支配的确定性任务, 因为这些任务可以清晰地定义ID/OOD, 并且便于直接的定量误差评估.

### 3.1 FUNDAMENTAL PHYSICAL SCENARIOS

1. 基本物理场景
   - 具体来说, 我们考虑了图2中所示的三个物理场景: 
     1. 匀速直线运动(Uniform Linear Motion): 一个彩色小球以恒定速度水平移动, 用于说明惯性定律(law of Inertia).
     2. 完全弹性碰撞(Perfectly Elastic Collision): 两个具有不同大小和速度的小球水平相向移动并发生碰撞, 其背后的物理定律是能量和动量守恒(conservation of energy and momentum).
     3. 抛物线运动(Parabolic Motion): 一个具有初始水平速度的小球由于重力作用下落, 代表牛顿第二运动定律(Newton’s second law of motion).
   - 每种运动由其初始帧决定.

2. 训练数据生成
   - 我们使用Box2D模拟不同场景下的运动状态, 并将其渲染为视频, 每个场景有2到4个自由度(degrees of freedom, DoF), 如小球的初速度和质量.
   - 我们为每个自由度定义了一个分布内的范围, 在这些范围内均匀采样高维网格, 生成了3万(30K)、30万(300K)和300万(3M)个视频训练数据集.
   - 所有小球的密度相同, 因此它们的质量由其大小推断得出.
   - 在抛物线运动中, 重力加速度是恒定的以保持一致性.
   - 初始小球的位置在可见范围内随机初始化. 更多细节请参考附录A.4.

3. 测试数据生成
   - 我们使用分布内和分布外的数据来评估训练好的模型.
   - 对于ID评估, 我们从训练时使用的相同网格中采样, 确保没有特定数据点属于训练集.
   - OOD评估视频则通过使用超出训练范围的初始半径和速度值生成.
   - 有多种类型的OOD设置, 例如仅速度或半径超出范围, 或者两者均超出. 详细信息请见附录A.4.

4. 模型训练
   - 对于每个场景, 我们从头开始训练不同规模的模型, 如表1所示. 这确保结果不受不可控的预训练数据影响.
   - 我们提供前三帧作为条件输入, 这足以推断小球的速度并预测后续帧.
   - 扩散模型(Diffusion model)在32个Nvidia A100 GPU上以批次大小256训练了10万(100K)步, 这已经足够使模型收敛. 经过30万(300K)步训练的模型表现相似.
   - 我们保持预训练的VAE固定不变. 每个视频由32帧组成, 分辨率为128x128.
   - 我们还尝试了256x256的分辨率, 尽管泛化误差相似, 但训练过程显著变慢.

5. 评估指标
   - 我们观察到学习到的模型能够生成形状一致的小球.
   - 为了获取生成视频中第 \(i\) 个小球的中心位置 \(x_{i,t}\), 我们使用了一种基于有色像素均值的启发式算法, 通过颜色来区分小球.
   - 为了确保位置 \(x_{i,t}\) 的正确性, 我们排除了部分小球超出视野的帧, 得到有效帧集合 \(T\).
   - 对于碰撞场景, 仅考虑碰撞后的帧.
   - 然后我们通过求导计算每个小球在每一时刻的速度 \(v_{i,t}\). 视频的误差定义为: 
   $$
   e = \frac{1}{N |T|} \sum_{i=1}^N \sum_{t \in T} \lVert v_{i,t} - \hat{v}_{i,t} \rVert
   $$
     其中, \(v_{i,t}\) 是在时刻 \(t\) 的计算速度, \(\hat{v}_{i,t}\) 是模拟器中的真实速度, \(N\) 是小球的数量, \(|T|\) 是有效帧的数量.

6. 基线(Baseline)
   - 我们计算了真实速度与从真实视频中解析出的值之间的误差, 称为“Groundtruth(GT)”.
   - 这代表了由于将视频解析为速度而引入的系统误差, 也定义了模型可以达到的最小误差.

### 3.2 MAIN RESULT OF SCALING DATA AND MODEL

1. 分布内泛化的主要结果
   - 如图3所示, 对于分布内(ID)泛化, 增加模型规模(从DiT-S到DiT-L)或数据量(从30K到3M)会一致地降低三个任务中的速度误差, 这强有力地证明了扩展对于ID泛化的重要性.
   - 以匀速运动任务为例: 使用30K数据时, DiT-S模型的速度误差为0.022, 而DiT-L在3M数据上的误差为0.012, 与从真实视频中获得的误差0.010非常接近.

2. 分布外泛化的不同结果
   - 然而, 对于分布外(OOD)预测, 结果则大不相同. 首先, 在所有设置中, OOD的速度误差比ID误差高一个数量级. 例如, 在使用3M数据进行匀速运动的DiT-L模型中, OOD误差为0.427, 而ID误差仅为0.012.
   - 其次, 增加训练数据和模型规模对减少OOD预测误差的影响很小, 甚至有负面影响. 速度误差的变化随着数据或模型规模的变化呈现高度随机性, 例如, 使用DiT-B在匀速运动任务中的误差分别为0.433、0.328和0.358, 对应数据量为30K、300K和3M.
   - 我们还在匀速运动3M数据集上训练了DiT-XL, 但未观察到在OOD泛化方面的改善. 因此, 受资源限制, 我们没有继续在其他场景或数据集上训练DiT-XL.
   - 这些发现表明, 扩展无法在OOD场景中实现推理. ID和OOD设置之间的显著差异进一步激发了我们在第5.2节中对视频生成泛化机制的研究.

## 4 COMBINATORIAL GENERALIZATION

在第3节中, 视频生成模型未能在OOD场景中进行推理. 这是可以理解的——从数据中推导出精确的物理定律对人类和模型来说都很困难. 例如, 科学家花了几个世纪才提出牛顿的三大运动定律. 然而, 即使是小孩子也可以通过结合过去的经验元素来直观地预测日常情况的结果. 这种结合已知信息以预测新场景的能力被称为组合泛化(combinatorial generalization). 在本节中, 我们评估基于扩散模型的视频模型的组合能力.

### 4.1 COMBINATORIAL PHYSICAL SCENARIOS

1. 选择PHYRE模拟器
   - 我们选择PHYRE模拟器(Bakhtin et al., 2019)作为我们的测试平台——这是一个二维(2D)环境, 涉及多个对象的自由下落并相互碰撞, 形成复杂的物理交互.
   - 其特点是具有多样的对象类型, 包括球、罐、条和墙, 这些对象可以是固定的或动态的. 这使得碰撞、抛物线轨迹、旋转和摩擦等复杂交互可以同时发生在一个视频中.
   - 尽管如此, 潜在的物理定律是确定性的, 使得模型可以学习这些定律并预测未见过的场景.

2. 训练数据
   - 我们考虑了八种类型的对象, 包括两个动态灰球、一组固定的黑球、一个固定的黑条、一个动态条、一组动态站立条、一个动态罐和一个动态站立棍.
   - 每个任务包含一个红球和从这八种类型中随机选择的四个对象, 形成了 \(C_4^8 = 70\) 种唯一模板. 示例请见图4.
   - 每个模板用四个对象的随机大小和位置初始化, 生成了10万(100K)个视频, 以涵盖可能场景的范围.
   - 为了探索模型的组合能力和扩展效果, 我们将训练数据分为三个级别: 包含所有类型两对象交互的最小集合(6个模板, 60万视频), 以及包含30个和60个模板的较大集合(分别为300万和600万视频), 60模板集合几乎覆盖了整个模板空间.
   - 最小训练集对模型的组合泛化能力提出了最高要求.

3. 测试数据
   - 对于每个训练模板, 我们保留了一小部分视频以创建模板内(in-template)评估集.
   - 此外, 保留了10个未使用的模板用于模板外(out-of-template)评估, 以评估模型对训练期间未见到的新组合的泛化能力.

4. 模型训练
   - 由于初始对象是静止的, 第一帧被用作视频生成的条件输入. 我们发现, 较小的模型(如DiT-S)在处理复杂视频时表现不佳, 因此主要使用DiT-B和DiT-XL.
   - 所有模型在64个Nvidia A100 GPU上进行了长达100万(1000K)梯度步的训练, 批次大小为256, 以确保接近收敛.
   - 为了更好地捕捉物理事件的复杂性, 我们将分辨率增加到256x256, 并包含32帧.

5. 评估指标
   - 我们使用了多种指标来评估生成视频与真实视频的相似度.
   - Frechet Video Distance(FVD)(Unterthiner et al., 2018): 使用在Kinetics-400上预训练的膨胀3D卷积网络(Inflated-3D ConvNets, I3D)(Carreira & Zisserman, 2017)计算生成视频与真实视频之间的特征距离.
   - SSIM和PSNR(Wang et al., 2004): 这是像素级指标. SSIM评估亮度、对比度和结构相似性, 而PSNR则衡量峰值信号与均方误差之间的比率, 两者均对帧进行平均.
   - LPIPS(Zhang et al., 2018): 评估图像块之间的感知相似性(perceptual similarity).
   - 我们还包括了人工评估, 报告了生成视频中违反物理定律的异常比例(abnormal ratio), 由人类评估.

### 4.2 MAIN RESULTS

1. 组合泛化的主要结果
   - 由于复杂性增加, 该任务需要更高的分辨率、更多的训练迭代和更大的模型规模以取得良好表现. 因此, 我们无法如第3节那样对所有数据和模型规模的组合进行全面扫描.
   - 因此, 我们从最大的模型DiT-XL开始, 研究组合泛化中的数据扩展行为. 如表2所示, 当模板数量从6增加到60时, 所有指标在模板外(out-of-template)测试集上都有所改善. 特别地, 人工评估的异常率从67%显著降低到10%.
   - 相反, 使用6个模板训练的模型在模板内(in-template)测试集上取得了最佳的SSIM、PSNR和LPIPS得分. 这可以解释为在6模板集合中, 每个训练样本的曝光次数比在60模板集合中多十倍, 使其能够更好地拟合与模板6相关的模板内任务.
   - 此外, 我们对完整的60模板进行了DiT-B模型的额外实验, 以验证模型扩展的重要性. 如预期的那样, 异常率增加到24%. 这些结果表明, 模型容量和组合空间的覆盖对组合泛化至关重要.
   - 这一见解表明, 视频生成的扩展规律应该集中于增加组合的多样性, 而不仅仅是增加数据量. 我们模型的视频生成可视化结果可以在图17和图18中找到.

## 5 DEEPER ANALYSIS

在本节中, 我们旨在通过系统的实验设计来调查视频生成模型的泛化机制. 基于这些发现, 我们尝试识别组合泛化中的某些模式, 这些模式可能有助于利用或提示模型的能力.

### 5.1 UNDERSTANDING GENERALIZATION FROM INTERPOLATION AND EXTRAPOLATION

1. 泛化能力的来源
   - 模型的泛化能力根源于其插值(interpolation)和外推(extrapolation)能力(Xu et al., 2020; Balestriero et al., 2021). 在本节中, 我们设计了实验来探索视频生成模型在这些能力方面的极限.
   - 我们设计了故意遗漏某些潜在值(如速度)的数据集. 在训练后, 我们测试模型在已见和未见场景中的预测表现, 主要关注匀速运动和碰撞过程.

2. 匀速运动
   - 我们创建了一系列训练集, 其中某个速度范围被省略. 每个训练集包含20万(200K)个视频以确保公平.
   - 如图5(1)-(2)所示, 如果训练集中存在较大的速度空缺, 模型在初始帧显示中等速度时, 往往会生成高或低速度以类似训练数据的表现.
   - 我们发现视频生成模型的OOD准确性与空缺的大小密切相关, 如图5(3)所示. 当空缺减少时, 模型可以正确插值大多数OOD数据.
   - 此外, 如图5(4)和(5)所示, 当重新引入缺失范围的一部分(不增加数据量), 模型表现出更强的插值能力.

3. 碰撞过程
   - 碰撞涉及多个变量, 更具挑战性, 因为模型必须学习一个二维的非线性函数.
   - 具体来说, 我们从两个小球的初速度训练集中排除一个或多个方形区域, 然后评估碰撞后的速度预测误差.
   - 对于每个速度点, 我们在半径参数的网格上采样, 生成多个视频案例并计算平均误差.
   - 如图6(1)-(2)所示, 发生了一种有趣的现象. 视频生成模型的外推误差在OOD点之间表现出一种有趣的差异: 对于位于训练集凸包(convex hull)内部的OOD速度组合(即黄色区域中的内部红色方块), 模型具有良好的泛化能力. 然而, 当潜在值位于训练集凸包的外部空间时, 模型的误差显著增加.

### 5.2 MEMORIZATION OR GENERALIZATION

1. 记忆还是泛化
   - 先前的研究(Hu et al., 2024)表明, 大型语言模型(LLMs)依赖于记忆, 在推理过程中重复训练案例, 而不是学习任务背后的规则, 如加法运算. 这导致它们对未见数据的泛化受到限制.
   - 在本节中, 我们研究视频生成模型是否表现出类似的行为, 即在数据上进行记忆而非理解物理定律.

2. 实验设计
   - 我们在速度范围 \(v \in [2.5, 4.0]\) 的匀速运动视频上训练模型, 使用前三帧作为输入条件.
   - 使用了两个训练集: Set-1仅包含从左向右移动的小球, 而Set-2则包括通过水平翻转在训练时引入的双向运动.
   - 在评估阶段, 我们重点关注训练数据中未包含的低速小球(\(v \in [1.0, 2.5]\)). 如图7所示, Set-1模型生成的所有视频仅具有正速度, 并偏向高速范围.
   - 相比之下, Set-2模型偶尔生成负速度的视频, 如绿色圆圈所示. 例如, 一个低速小球从左向右移动, 可能在条件帧之后突然反向运动.
   - 这可能是因为模型将反转的视频识别为低速小球的最相似匹配. 这两个模型之间的区别表明视频生成模型受训练数据中的“欺骗性”示例的影响.
   - 模型似乎依赖于记忆和基于案例的模仿, 而不是抽象出普遍的规则, 用于OOD泛化.

### 5.3 HOW DOES DIFFUSION MODEL RETRIEVE DATA?

1. 研究目标
   - 我们的目标是调查视频模型如何进行案例匹配(case matching)——即如何为给定的测试输入识别相近的训练示例.
   - 我们使用匀速直线运动进行本研究. 具体来说, 我们比较四个属性, 即颜色(color)、形状(shape)、大小(size)和速度(velocity), 以两两比较的方式进行.
   - 通过比较, 我们试图确定模型在案例匹配中更倾向于依赖哪些属性.

2. 属性组合实验
   - 每个属性有两个不相交的值集合. 对于每对属性, 共有四种类型的组合. 我们使用其中的两种组合进行训练, 剩下的两种用于测试.
   - 例如, 我们在图8(1)中比较颜色和形状. 训练时, 使用了红色球和蓝色方块的视频, 且它们具有相同范围的大小和速度. 在测试时, 蓝色球在条件帧之后立即变成方块, 而红色方块变成球.
   - 在1400个测试案例中没有发现例外, 这表明模型在案例匹配中优先考虑颜色而非形状.
   - 类似的趋势也出现在大小与形状、速度与形状的比较中, 如图8(2)-(3)所示, 表明形状是最不被优先考虑的属性.
   - 这表明基于扩散的视频模型本质上优先考虑形状之外的其他属性, 这也可能解释了当前开放集视频生成模型通常难以保持形状一致性的原因.

3. 其他属性对的比较
   - 图9展示了其他三对属性的实验结果.
   - 对于速度与大小的比较, 组合泛化的表现出奇地好. 模型在大多数超出训练分布的测试案例中有效地保持了初始大小和速度.
   - 然而, 模型表现出对大小略微优于速度的偏好, 特别是在半径和速度值极端的情况下(如图9(1)中左上和右下的位置).
   - 如图9(2)所示, 颜色(color)可以在大多数情况下与大小(size)组合.
   - 相反, 对于图9(3)中的颜色与速度(velocity)的比较, 训练时使用了高速蓝色小球和低速红色小球. 在测试时, 低速蓝色小球表现得比其条件速度要快得多.
   - 测试集中没有小球改变其颜色, 这表明模型优先考虑颜色而非速度.
   - 基于上述分析, 我们可以得出结论: 在案例匹配中, 颜色属性在多个组合中被模型优先考虑.

### 5.4 HOW DOES COMPLEX COMBINATORIAL GENERALIZATION HAPPEN?

1. 组合泛化的复杂性
   - 在第4节中, 我们展示了扩展数据覆盖范围可以提升组合泛化能力. 但究竟什么样的数据能够真正实现概念上的可组合视频生成呢？
   - 在本节中, 我们通过实验设计识别了三种基本的组合模式.

2. 属性组合(Attribute composition)
   - 如图9(1)-(2)所示, 某些属性对(如速度与大小, 颜色与大小)表现出了一定程度的组合泛化能力.

3. 空间组合(Spatial composition)
   - 如附录中图11(左侧)所示, 训练数据包含了两种不同类型的物理事件. 一种类型涉及蓝色方块以恒定速度水平移动, 而红色小球保持静止. 另一种类型描绘了红色小球朝向并反弹墙面, 而蓝色方块保持静止.
   - 在测试时, 当红色小球和蓝色方块同时移动时, 学习到的模型能够生成红色小球反弹墙面且蓝色方块保持匀速运动的场景.

4. 时间组合(Temporal combination)
   - 如图11右侧所示, 当训练数据包含不同的物理事件时——一半是两个小球碰撞但不反弹, 另一半是红色小球反弹墙面——模型学会了将这些事件在时间上进行组合.
   - 因此, 在评估时, 当小球在墙附近碰撞时, 模型能够准确预测碰撞并确定蓝色小球将以不变的速度反弹离开墙面.
   - 通过这些属性、空间和时间的组合模式, 视频生成模型可以识别训练集中的基本物理事件, 并在属性、时间和空间上对它们进行组合, 从而生成包含复杂物理事件链的视频.

### 5.5 IS VIDEO SUFFICIENT FOR COMPLETE PHYSICS MODELING?

1. 视频是否足够用于完整的物理建模？
   - 要使视频生成模型作为世界模型(world model)发挥作用, 视觉表示必须提供足够的信息来进行完整的物理建模.
   - 在我们的实验中, 我们发现视觉模糊性会导致精细物理建模中的显著不准确性.
   - 例如, 在图10中, 仅凭视觉很难判断一个小球是否能通过间隙, 当大小差异在像素级别时, 这会导致在视觉上合理但实际上错误的结果.
   - 类似地, 小球相对于一个块的水平位置的视觉模糊性也可能导致不同的结果.
   - 这些发现表明, 单纯依靠视觉表示可能不足以进行精确的物理建模.
